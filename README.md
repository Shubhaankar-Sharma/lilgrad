# lilgrad

A work-in-progress tiny deep learning library (from scratch) in Python.

Checkout [mnist example](./mnist.ipynb)

#### Disclaimer
This is very work-in-progress. The nn engine assumes classification tasks for now, and the activation functions are hardcoded to ReLU and Softmax, and the Loss function to Cross Entropy Loss.

The library is a mess in terms of code because I didn't take inspiration from any existing codebases yet and just wrote the math how I think it should work. I will be cleaning it up and adding more features as I learn more about deep learning, and optimization techniques.

In the future this also might include transformers, attention mechanisms, and other cool stuff (maybe? but will have to use gpu for that).
